{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP8iXbF1zmI2Z+obXJaDsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matheus-brazao/Project-DIO-DatasetIA/blob/main/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcNi62hbsA1L"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Section 0 — Setup & Imports\n",
        "# =========================\n",
        "# Original: import keras (standalone, legacy), manual loaders\n",
        "# Update: use TF 2.x (tf.keras), with TFDS or directory loader\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout)\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "# ================\n",
        "# Section 1 — Config\n",
        "# ================\n",
        "# Change this to switch dataset quickly:\n",
        "# - \"tfds:cats_vs_dogs\" (uses TFDS)\n",
        "# - \"dir:/content/drive/MyDrive/my_dataset\" (directory with subfolders per class)\n",
        "DATA_SOURCE = \"tfds:cats_vs_dogs\"\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "\n",
        "# FAST_MODE: tiny run for quick sanity-check (few batches/epochs)\n",
        "FAST_MODE = True\n",
        "EPOCHS_BASELINE = 2 if FAST_MODE else 5\n",
        "EPOCHS_TRANSFER = 2 if FAST_MODE else 5\n",
        "STEPS_PER_EPOCH = 5 if FAST_MODE else 100\n",
        "VAL_STEPS = 2 if FAST_MODE else 30\n",
        "TEST_STEPS = 2 if FAST_MODE else 50\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Section 2 — Dataset loaders (TFDS or folder)\n",
        "# ==========================================\n",
        "def preprocess_baseline(image, label):\n",
        "    # Baseline: simple [0,1] normalization\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "def preprocess_for_vgg(image, label):\n",
        "    # VGG16 expects its own preprocess_input\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, tf.float32)  # will be scaled by preprocess_input\n",
        "    image = preprocess_input(image)     # center/scale as VGG16 expects\n",
        "    return image, label\n",
        "\n",
        "def load_tfds(name=\"cats_vs_dogs\"):\n",
        "    (ds_train, ds_val, ds_test), info = tfds.load(\n",
        "        name,\n",
        "        split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
        "        as_supervised=True,\n",
        "        with_info=True\n",
        "    )\n",
        "    num_classes = info.features[\"label\"].num_classes\n",
        "    class_names = info.features[\"label\"].names\n",
        "    return ds_train, ds_val, ds_test, num_classes, class_names\n",
        "\n",
        "def load_from_directory(dir_path):\n",
        "    # Directory structure:\n",
        "    # dir_path/\n",
        "    #   class_a/ img1.jpg ...\n",
        "    #   class_b/ ...\n",
        "    train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        dir_path,\n",
        "        validation_split=0.2, subset=\"training\", seed=SEED,\n",
        "        image_size=IMG_SIZE, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    val = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        dir_path,\n",
        "        validation_split=0.2, subset=\"validation\", seed=SEED,\n",
        "        image_size=IMG_SIZE, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    class_names = train.class_names\n",
        "    num_classes = len(class_names)\n",
        "    # Create a small test set from val (quick hack for demo)\n",
        "    test = val.unbatch().batch(BATCH_SIZE)\n",
        "    return train, val, test, num_classes, class_names\n",
        "\n",
        "def build_pipelines(DATA_SOURCE):\n",
        "    if DATA_SOURCE.startswith(\"tfds:\"):\n",
        "        name = DATA_SOURCE.split(\"tfds:\")[1]\n",
        "        ds_train, ds_val, ds_test, num_classes, class_names = load_tfds(name)\n",
        "        # Two parallel pipelines: baseline and transfer (different preprocessing)\n",
        "        train_base = ds_train.map(preprocess_baseline, num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "        val_base   = ds_val.map(preprocess_baseline,   num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "        test_base  = ds_test.map(preprocess_baseline,  num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "        train_vgg = ds_train.map(preprocess_for_vgg, num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "        val_vgg   = ds_val.map(preprocess_for_vgg,   num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "        test_vgg  = ds_test.map(preprocess_for_vgg,  num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    elif DATA_SOURCE.startswith(\"dir:\"):\n",
        "        dir_path = DATA_SOURCE.split(\"dir:\")[1]\n",
        "        raw_train, raw_val, raw_test, num_classes, class_names = load_from_directory(dir_path)\n",
        "\n",
        "        # Baseline pipelines\n",
        "        train_base = raw_train.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y)).cache().prefetch(AUTOTUNE)\n",
        "        val_base   = raw_val.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y)).cache().prefetch(AUTOTUNE)\n",
        "        test_base  = raw_test.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y)).cache().prefetch(AUTOTUNE)\n",
        "\n",
        "        # VGG pipelines\n",
        "        train_vgg = raw_train.map(lambda x,y: (preprocess_input(tf.cast(x, tf.float32)), y)).cache().prefetch(AUTOTUNE)\n",
        "        val_vgg   = raw_val.map(lambda x,y: (preprocess_input(tf.cast(x, tf.float32)), y)).cache().prefetch(AUTOTUNE)\n",
        "        test_vgg  = raw_test.map(lambda x,y: (preprocess_input(tf.cast(x, tf.float32)), y)).cache().prefetch(AUTOTUNE)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"DATA_SOURCE must start with 'tfds:' or 'dir:'\")\n",
        "\n",
        "    # FAST subsets (few steps) for quick runs\n",
        "    train_base_small = train_base.take(STEPS_PER_EPOCH)\n",
        "    val_base_small   = val_base.take(VAL_STEPS)\n",
        "    test_base_small  = test_base.take(TEST_STEPS)\n",
        "\n",
        "    train_vgg_small = train_vgg.take(STEPS_PER_EPOCH)\n",
        "    val_vgg_small   = val_vgg.take(VAL_STEPS)\n",
        "    test_vgg_small  = test_vgg.take(TEST_STEPS)\n",
        "\n",
        "    return (train_base_small, val_base_small, test_base_small,\n",
        "            train_vgg_small,  val_vgg_small,  test_vgg_small,\n",
        "            num_classes, class_names)\n",
        "\n",
        "(train_base, val_base, test_base,\n",
        " train_vgg,  val_vgg,  test_vgg,\n",
        " NUM_CLASSES, CLASS_NAMES) = build_pipelines(DATA_SOURCE)\n",
        "\n",
        "print(\"Classes:\", CLASS_NAMES, \"| NUM_CLASSES:\", NUM_CLASSES)\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# Section 3 — Baseline model (from scratch)\n",
        "# =========================================\n",
        "def build_baseline(input_shape=IMG_SIZE+(3,), num_classes=2):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, 3, activation=\"relu\", input_shape=input_shape),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(64, 3, activation=\"relu\"),\n",
        "        MaxPooling2D(),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "baseline = build_baseline(num_classes=NUM_CLASSES)\n",
        "baseline.summary()\n",
        "\n",
        "history_base = baseline.fit(\n",
        "    train_base,\n",
        "    validation_data=val_base,\n",
        "    epochs=EPOCHS_BASELINE,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_steps=VAL_STEPS,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# Section 4 — Transfer learning (VGG16)\n",
        "# =========================================\n",
        "# Original: include_top=True then swap last Dense.\n",
        "# Update: include_top=False and add our 2-class head.\n",
        "\n",
        "base_model = VGG16(include_top=False, weights=\"imagenet\", input_shape=IMG_SIZE+(3,))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # freeze feature extractor\n",
        "\n",
        "inputs = tf.keras.Input(shape=IMG_SIZE+(3,))\n",
        "x = inputs\n",
        "x = base_model(x, training=False)\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "outputs = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "transfer_model = Model(inputs, outputs, name=\"vgg16_transfer\")\n",
        "transfer_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                       loss=\"sparse_categorical_crossentropy\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "transfer_model.summary()\n",
        "\n",
        "history_transfer = transfer_model.fit(\n",
        "    train_vgg,\n",
        "    validation_data=val_vgg,\n",
        "    epochs=EPOCHS_TRANSFER,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_steps=VAL_STEPS,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# Section 5 — Quick comparison & test eval\n",
        "# =========================================\n",
        "plt.plot(history_base.history[\"val_accuracy\"], label=\"baseline val_acc\")\n",
        "plt.plot(history_transfer.history[\"val_accuracy\"], label=\"transfer val_acc\")\n",
        "plt.xlabel(\"epochs\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.show()\n",
        "\n",
        "print(\"Baseline test (subset):\")\n",
        "print(baseline.evaluate(test_base, verbose=0))\n",
        "\n",
        "print(\"Transfer test (subset):\")\n",
        "print(transfer_model.evaluate(test_vgg, verbose=0))\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# Section 6 — Tiny manual prediction (2 images)\n",
        "# =========================================\n",
        "# Take just 2 samples from the test_vgg pipeline and predict\n",
        "for images, labels in test_vgg.take(1):\n",
        "    preds = transfer_model.predict(images[:2])\n",
        "    import numpy as np\n",
        "    pred_ids = np.argmax(preds, axis=1)\n",
        "    for i in range(2):\n",
        "        plt.imshow((images[i].numpy() - images[i].numpy().min()) / (images[i].numpy().ptp()+1e-6))\n",
        "        plt.title(f\"Pred: {CLASS_NAMES[pred_ids[i]]} | True: {CLASS_NAMES[int(labels[i])]}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n"
      ]
    }
  ]
}